@article{Ding2023,
 abstract = {With the prevalence of pre-trained language models (PLMs) and the pre-training--fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term `delta-tuning', where `delta' a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are `changed' during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.},
 author = {Ding, Ning
and Qin, Yujia
and Yang, Guang
and Wei, Fuchao
and Yang, Zonghan
and Su, Yusheng
and Hu, Shengding
and Chen, Yulin
and Chan, Chi-Min
and Chen, Weize
and Yi, Jing
and Zhao, Weilin
and Wang, Xiaozhi
and Liu, Zhiyuan
and Zheng, Hai-Tao
and Chen, Jianfei
and Liu, Yang
and Tang, Jie
and Li, Juanzi
and Sun, Maosong},
 day = {01},
 doi = {10.1038/s42256-023-00626-4},
 issn = {2522-5839},
 journal = {Nature Machine Intelligence},
 month = {Mar},
 number = {3},
 pages = {220-235},
 title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
 url = {https://doi.org/10.1038/s42256-023-00626-4},
 volume = {5},
 year = {2023}
}
