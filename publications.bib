@article{Ding2023,
    author={Ding, Ning
    and Qin, Yujia
    and Yang, Guang
    and Wei, Fuchao
    and Yang, Zonghan
    and Su, Yusheng
    and Hu, Shengding
    and Chen, Yulin
    and Chan, Chi-Min
    and Chen, Weize
    and Yi, Jing
    and Zhao, Weilin
    and Wang, Xiaozhi
    and Liu, Zhiyuan
    and Zheng, Hai-Tao
    and Chen, Jianfei
    and Liu, Yang
    and Tang, Jie
    and Li, Juanzi
    and Sun, Maosong},
    title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
    journal={Nature Machine Intelligence},
    year={2023},
    month={Mar},
    day={01},
    volume={5},
    number={3},
    pages={220-235},
    abstract={With the prevalence of pre-trained language models (PLMs) and the pre-training--fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term `delta-tuning', where `delta' a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are `changed' during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.},
    issn={2522-5839},
    doi={10.1038/s42256-023-00626-4},
    url={https://doi.org/10.1038/s42256-023-00626-4}
}
@inproceedings{Yang2023, 
    title={Video Event Extraction via Tracking Visual States of Arguments}, 
    volume={37}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/25418}, 
    DOI={10.1609/aaai.v37i3.25418}, 
    abstractNote={Video event extraction aims to detect salient events from a video and identify the arguments for each event as well as their semantic roles. Existing methods focus on capturing the overall visual scene of each frame, ignoring fine-grained argument-level information. Inspired by the definition of events as changes of states, we propose a novel framework to detect video events by tracking the changes in the visual states of all involved arguments, which are expected to provide the most informative evidence for the extraction of video events. In order to capture the visual state changes of arguments, we decompose them into changes in pixels within objects, displacements of objects, and interactions among multiple arguments. We further propose Object State Embedding, Object Motion-aware Embedding and Argument Interaction Embedding to encode and track these changes respectively. Experiments on various video event extraction tasks demonstrate significant improvements compared to state-of-the-art models. In particular, on verb classification, we achieve 3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation Recognition. Our Code is publicly available at https://github.com/Shinetism/VStates for research purposes.}, 
    number={3}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Yang, Guang and Li, Manling and Zhang, Jiajie and Lin, Xudong and Ji, Heng and Chang, Shih-Fu}, 
    year={2023}, 
    month={Jun.},
    pages={3136-3144} 
}
@inproceedings{yang2024completeomrsolution,
  author       = {Yang, Guang and
                  Zhang, Muru and
                  Qiu, Lin and
                  Wan, Yanming and
                  Smith, Noah A.},
  title        = {Toward a More Complete OMR Solution},
  journal    = {Proceedings of the 25th International Society for
                   Music Information Retrieval Conference
                  },
  year         = 2024,
  pages        = {930-937},
  publisher    = {ISMIR},
  month        = nov,
  venue        = {San Francisco, California, USA and Online},
  doi          = {10.5281/zenodo.14877483},
  url          = {https://doi.org/10.5281/zenodo.14877483},
}
@misc{yang2025legatolargescaleendtoendgeneralizable,
      title={LEGATO: Large-scale End-to-end Generalizable Approach to Typeset OMR}, 
      author={Yang, Guang and Ebert, Victoria and Tamer, Nazif and Pozzobon, Luiza and Smith, Noah A.},
      year={2025},
      eprint={2506.19065},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.19065}, 
}
